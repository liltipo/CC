{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1> ILI285 - Computación Científica I  / INF285 - Computación Científica </h1>\n",
    "    <h2> Conjugate Gradient Method </h2>\n",
    "    <h2> <a href=\"#acknowledgements\"> [S]cientific [C]omputing [T]eam </a> </h2>\n",
    "    <h2> Version: 1.16</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Introduction](#intro)\n",
    "* [Gradient Descent](#GDragon)\n",
    "* [Conjugate Gradient Method](#CGM)\n",
    "* [Let's Play: Practical Exercises and Profiling](#LP)\n",
    "* [Acknowledgements](#acknowledgements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import solve_triangular\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "# pip install memory_profiler\n",
    "%load_ext memory_profiler\n",
    "np.random.seed(0)\n",
    "from ipywidgets import interact, IntSlider\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.size'] = 14\n",
    "mpl.rcParams['axes.labelsize'] = 20\n",
    "mpl.rcParams['xtick.labelsize'] = 14\n",
    "mpl.rcParams['ytick.labelsize'] = 14\n",
    "\n",
    "def plot_matrices_with_values(ax,M,flag_values):\n",
    "    N=M.shape[0]\n",
    "    cmap = plt.get_cmap('GnBu')\n",
    "    ax.matshow(M, cmap=cmap)\n",
    "    if flag_values:\n",
    "        for i in np.arange(0, N):\n",
    "            for j in np.arange(0, N):\n",
    "                ax.text(i, j, '{:.2f}'.format(M[i,j]), va='center', ha='center', color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='intro' />\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to another edition of our Jupyter Notebooks. Here, we'll teach you how to solve $A\\,x = b$ with $A$ being a _symmetric positive-definite matrix_, but the following methods have a key difference with the previous ones: these do not depend on a matrix factorization. The two methods that we'll see are called the Gradient Descent and the Conjugate Gradient Method. On the latter, we'll also see the benefits of preconditioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='GDragon' />\n",
    "\n",
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an iterative method. If you remember the iterative methods in the previous Notebook, to find the next approximate solution $\\mathbf{x}_{k+1}$ you'd add a vector to the current approximate solution, $\\mathbf{x}_k$, that is: $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\text{vector}$. In this method, $\\text{vector}$ is $\\alpha_{k}\\,\\mathbf{r}_k$, where $\\mathbf{r}_k$ is the residue ($\\mathbf{b} - A\\,\\mathbf{x}_k$) and $\\alpha_k = \\cfrac{(\\mathbf{r}_k)^T\\,\\mathbf{r}_k}{(\\mathbf{r}_k)^T\\,A\\,\\mathbf{r}_k}$, starting with some initial guess $\\mathbf{x}_0$. Let's look at the implementation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(A, b, x0, n_iter=10, tol=1e-10):\n",
    "    n = A.shape[0]\n",
    "    #array with solutions\n",
    "    X = np.full((n_iter, n),np.nan)\n",
    "    X[0] = x0\n",
    "\n",
    "    for k in range(1, n_iter):\n",
    "        r = b - np.dot(A, X[k-1])\n",
    "        if np.linalg.norm(r)<tol: # The algorithm \"converged\"\n",
    "            X[k:] = X[k-1]\n",
    "            return X\n",
    "            break\n",
    "        alpha = np.dot(r, r)/np.dot(r, np.dot(A, r))\n",
    "        X[k] = X[k-1] + alpha*r\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try our algorithm! But first, let's borrow a function to generate a random symmetric positive-definite matrix, kindly provided by the previous notebook, and another one to calculate the vectorized euclidean metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Randomly generates an nxn symmetric positive-\n",
    "definite matrix A.\n",
    "\"\"\"\n",
    "def generate_spd_matrix(n):\n",
    "    A = np.random.random((n,n))\n",
    "    #constructing symmetry\n",
    "    A += A.T\n",
    "    #symmetric+diagonally dominant -> symmetric positive-definite\n",
    "    deltas = 0.1*np.random.random(n)\n",
    "    row_sum = A.sum(axis=1)-np.diag(A)\n",
    "    np.fill_diagonal(A, row_sum+deltas)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try our algorithm with some matrices of different sizes, and we'll compare it with the solution given by Numpy's solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8680aa0350b4c2dacfe1b742b0ec9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='n_size', max=50, min=3), IntSlider(value=10, description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_small_example_GD(n_size=3, n_iter=10)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_small_example_GD(n_size=3, n_iter=10):\n",
    "    np.random.seed(0)\n",
    "    A = generate_spd_matrix(n_size)\n",
    "    b = np.ones(n_size)\n",
    "    x0 = np.zeros(n_size)\n",
    "\n",
    "    X = gradient_descent(A, b, x0, n_iter)\n",
    "    sol = np.linalg.solve(A, b)\n",
    "    print('Gradiente descent : ',X[-1])\n",
    "    print('np solver         : ',sol)\n",
    "    print('norm(difference): \\t',np.linalg.norm(X[-1] - sol)) # difference between gradient_descent's solution and Numpy's solver solution\n",
    "interact(show_small_example_GD,n_size=(3,50,1),n_iter=(5,50,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we're getting ok solutions with 15 iterations, even for larger matrices. \n",
    "A variant of this method is currently used in training neural networks and in Data Science in general, the main difference is that they call the \\alpha parameter 'learning rate' and keep it constant.\n",
    "Another important reason is that sometimes in Data Science they need to solve a nonlinear system of equations rather than a linear one, the good thing is that to solve nonlinear system of equations we do it by a sequence of linear system of equations!\n",
    "Now, we will discuss a younger sibling, the Conjugate Gradient Method, which is the prefered when the associated matrix is symmetric and positive definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='CGM' />\n",
    "\n",
    "## Conjugate Gradient Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method works by succesively eliminating the $n$ orthogonal components of the error, one by one. The method arrives at the solution with the following finite loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(A, b, x0, full_output=False, tol=1e-16):\n",
    "    n = A.shape[0]\n",
    "    X = np.full((n+1, n),np.nan) # Storing partial solutions x_i\n",
    "    R = np.full((n+1, n),np.nan) # Storing residues r_i=b-A\\,x_i\n",
    "    D = np.full((n+1, n),np.nan) # Storing conjugate directions d_i\n",
    "    alphas = np.full(n,np.nan)   # Storing alpha's\n",
    "    betas = np.full(n,np.nan)    # Storing beta's\n",
    "    X[0] = x0                    # initial guess: x_0\n",
    "    R[0] = b - np.dot(A, x0)     # initial residue: r_0=b-A\\,x_0\n",
    "    D[0] = R[0]                  # initial direction: d_0\n",
    "    n_residuals = np.full(n+1,np.nan) # norm of residuals over iteration: ||r_i||_2\n",
    "\n",
    "    n_residuals[0] = np.linalg.norm(R[0]) # initilizing residual: ||r_0||_2\n",
    "    x_sol=x0                     # first approximation of solution\n",
    "    \n",
    "    for k in np.arange(n):\n",
    "        if np.linalg.norm(R[k])<=tol: # The algorithm converged\n",
    "            if full_output:\n",
    "                return X[:k+1], D[:k+1], R[:k+1], alphas[:k+1], betas[:k+1], n_residuals[:k+1]\n",
    "            else:\n",
    "                return x_sol\n",
    "        # This is the 'first' version of the algorithm\n",
    "        alphas[k] = np.dot(D[k], R[k]) / np.dot(D[k], np.dot(A, D[k]))\n",
    "        X[k+1] = X[k] + alphas[k]*D[k]\n",
    "        R[k+1] = R[k] - alphas[k]*np.dot(A, D[k])\n",
    "        n_residuals[k+1] = np.linalg.norm(R[k+1])\n",
    "        betas[k] = np.dot(D[k],np.dot(A,R[k+1]))/np.dot(D[k],np.dot(A,D[k]))\n",
    "        D[k+1] = R[k+1] - betas[k]*D[k]\n",
    "        x_sol=X[k+1]\n",
    "        \n",
    "    if full_output:\n",
    "        return X, D, R, alphas, betas, n_residuals\n",
    "    else:\n",
    "        return x_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function computes the A-inner product \n",
    "# between each pair of vectors provided in V. \n",
    "# If 'A' is not provided, it becomes the \n",
    "# traditional inner product.\n",
    "def compute_A_orthogonality(V,A='identity'):\n",
    "    m = V.shape[0]\n",
    "    n = V.shape[1]\n",
    "    \n",
    "    if isinstance(A, str):\n",
    "        A=np.eye(n)\n",
    "    \n",
    "    output =  np.full((m-1,m-1),np.nan)\n",
    "    \n",
    "    for i in range(m-1):\n",
    "        for j in range(m-1):\n",
    "            output[i,j]=np.dot(V[i],np.dot(A,V[j]))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad0c24a10874929a7f7c9bc560004fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='n_size', max=50, min=2), Checkbox(value=False, descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_small_example_CG(n_size=2, flag_image=False, flag_image_values=True)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_small_example_CG(n_size=2,flag_image=False,flag_image_values=True):\n",
    "    np.random.seed(0)\n",
    "    A = generate_spd_matrix(n_size)\n",
    "    b = np.ones(n_size)\n",
    "    x0 = np.zeros(n_size)\n",
    "\n",
    "    X, D, R, alphas, betas, n_residuals = conjugate_gradient(A, b, x0, True)\n",
    "    \n",
    "    if flag_image:\n",
    "        outR=compute_A_orthogonality(R)\n",
    "        outD=compute_A_orthogonality(D,A)\n",
    "        M=8\n",
    "        fig, ((ax1, ax2), (ax3, ax4))  = plt.subplots(2, 2, figsize=(2*M,M))\n",
    "        plot_matrices_with_values(ax1,np.log10(np.abs(outR)+1e-16),flag_image_values)\n",
    "        ax1.set_title(r'$\\log_{10}(|\\mathbf{r}_i^T \\, \\mathbf{r}_j|+10^{-16})$',pad=20)\n",
    "        plot_matrices_with_values(ax2,np.log10(np.abs(outD)+1e-16),flag_image_values)\n",
    "        ax2.set_title(r'$\\log_{10}(|\\mathbf{d}_i^T\\,A\\,\\mathbf{d}_j|+10^{-16})$',pad=20)\n",
    "        plt.sca(ax3)\n",
    "        plt.semilogy(n_residuals,'.')\n",
    "        plt.grid(True)\n",
    "        plt.ylabel(r'$||\\mathbf{r}_i||$')\n",
    "        plt.xlabel(r'$i$')\n",
    "        plt.title('n= %d'%n_size)\n",
    "        plt.sca(ax4)\n",
    "        plt.plot(alphas,'.',label=r'$\\alpha_i$',markersize=10)\n",
    "        plt.plot(betas,'.',label=r'$\\beta_i$',markersize=10)\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.xlabel(r'$i$')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('n_residuals:')\n",
    "        print(n_residuals)\n",
    "        print('alphas:')\n",
    "        print(alphas)\n",
    "        print('betas:')\n",
    "        print(betas)\n",
    "        print('R:')\n",
    "        print(R)\n",
    "        print('X:')\n",
    "        print(X)\n",
    "        print('D:')\n",
    "        print(D)\n",
    "interact(show_small_example_CG,n_size=(2,50,1),flag_image=False,flag_image_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0271fef69de4493fa4aebd8fe710d830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='n', max=3), IntSlider(value=-180, description='elev', ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(n, elev, azim)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_iterative_solution(A,b,X,R,D,n=0,elev=30,azim=310):\n",
    "    L=lambda x: np.dot(x,np.dot(A,x))-np.dot(b,x)\n",
    "    \n",
    "    fig=plt.figure(figsize=(20,10))\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "    \n",
    "    # Plotting the residual vectors\n",
    "    for v in R[:n+1]:\n",
    "        # We use ax1 for the actual values and ax1 for the normalized values.\n",
    "        # We normalize it just for plotting purposes, otherwise the last\n",
    "        # vectors look too tiny.\n",
    "        ax1.quiver(0, 0, 0, v[0], v[1], v[2],color='blue')\n",
    "        ax2.quiver(0, 0, 0, v[0]/np.linalg.norm(v), v[1]/np.linalg.norm(v), v[2]/np.linalg.norm(v),color='blue')\n",
    "    # Plotting the residual vectors\n",
    "    for v in X[1:n+1]:\n",
    "        ax1.quiver(0, 0, 0, v[0], v[1], v[2],color='red')\n",
    "        ax2.quiver(0, 0, 0, v[0]/np.linalg.norm(v), v[1]/np.linalg.norm(v), v[2]/np.linalg.norm(v),color='red')\n",
    "    # Plotting the direction vectors\n",
    "    for v in D[:n]:\n",
    "        ax1.quiver(0, 0, 0, v[0], v[1], v[2],color='green',linewidth=10,alpha=0.5)\n",
    "        ax2.quiver(0, 0, 0, v[0]/np.linalg.norm(v), v[1]/np.linalg.norm(v), \n",
    "                   v[2]/np.linalg.norm(v),color='green',linewidth=10,alpha=0.5)\n",
    "    \n",
    "    # plotting evolution of solution\n",
    "    v = X[0]\n",
    "    ax1.quiver(0, 0, 0, v[0], v[1], v[2], color='black', linestyle='dashed')\n",
    "    ax2.quiver(0, 0, 0, v[0]/np.linalg.norm(v), v[1]/np.linalg.norm(v), v[2]/np.linalg.norm(v),color='black',linestyle='dashed')\n",
    "    for k in np.arange(1,n+1):\n",
    "        v = X[k]-X[k-1]\n",
    "        vp= X[k-1]\n",
    "        ax1.quiver(vp[0], vp[1], vp[2], v[0], v[1], v[2], color='magenta',linewidth=10,alpha=0.5)\n",
    "        v = X[k]/np.linalg.norm(X[k])-X[k-1]/np.linalg.norm(X[k-1])\n",
    "        vp= X[k-1]/np.linalg.norm(X[k-1])\n",
    "        ax2.quiver(vp[0], vp[1], vp[2], v[0], v[1], v[2],color='magenta',linewidth=10,alpha=0.5)\n",
    "        \n",
    "    \n",
    "    #for v in X[]\n",
    "    ax1.set_xlim(min(0,np.min(X[:,0]),np.min(R[:,0])),max(0,np.max(X[:,0]),np.max(R[:,0])))\n",
    "    ax1.set_ylim(min(0,np.min(X[:,1]),np.min(R[:,1])),max(0,np.max(X[:,1]),np.max(R[:,1])))\n",
    "    ax1.set_zlim(min(0,np.min(X[:,2]),np.min(R[:,2])),max(0,np.max(X[:,2]),np.max(R[:,2])))\n",
    "    ax2.set_xlim(-1,1)\n",
    "    ax2.set_ylim(-1,1)\n",
    "    ax2.set_zlim(-1,1)\n",
    "    #fig.tight_layout()\n",
    "    ax1.view_init(elev,azim)\n",
    "    ax2.view_init(elev,azim)\n",
    "    plt.title('r-blue, x-red, d-green, x-mag, x0-black')\n",
    "    plt.show()\n",
    "\n",
    "# Setting a standard name for the variables\n",
    "np.random.seed(0)\n",
    "A = generate_spd_matrix(3)\n",
    "b = np.ones(3)\n",
    "x0 = np.ones(3)\n",
    "X, D, R, alphas, betas, n_residuals = conjugate_gradient(A, b, x0, True)\n",
    "# For plotting with widgets\n",
    "n_widget = IntSlider(min=0, max=b.shape[0], step=1, value=0)\n",
    "elev_widget = IntSlider(min=-180, max=180, step=10, value=-180)\n",
    "azim_widget = IntSlider(min=0, max=360, step=10, value=30)\n",
    "solution_evolution = lambda n,elev,azim: plot_iterative_solution(A,b,X,R,D,n,elev,azim)\n",
    "interact(solution_evolution,n=n_widget,elev=elev_widget,azim=azim_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The science behind this algorithm is in the classnotes and in the textbook (Numerical Analysis, 2nd Edition, Timothy Sauer). Now let's try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some questions to think about:\n",
    "* What are the advantages and disadvantages of each method: `gradient_descent` and `conjugate_gradient`?\n",
    "* In which cases can the Conjugate Gradient Method converge in less than $n$ iterations?\n",
    "* What will happen if you use the Gradient Descent or Conjugate Gradient Method with non-symmetric, non-positive-definite matrices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='LP' />\n",
    "\n",
    "## Let's Play: Practical Exercises and Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, define a function to calculate the progress of the relative error for a given method, that is, input the array of approximate solutions `X` and the real solution provided by Numpy's solver `r_sol` and return an array with the relative error for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_error(X, r_sol):\n",
    "    n_steps = X.shape[0]\n",
    "    n_r_sol = np.linalg.norm(r_sol)\n",
    "    E = np.zeros(n_steps)\n",
    "    for i in range(n_steps):\n",
    "        E[i] = np.linalg.norm(X[i] - r_sol) / n_r_sol\n",
    "    return E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying the two methods with a small non-symmetric, non-positive-definite matrix and plotting the forward error for all the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c766cf3bfc144168836f6ae4e9fff57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='np_seed'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_output_for_non_symmetric_and_npd(np_seed=0)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_output_for_non_symmetric_and_npd(np_seed=0):\n",
    "    np.random.seed(np_seed)\n",
    "    n = 10\n",
    "    A = 10 * np.random.random((n,n))\n",
    "    b = 10 * np.random.random(n)\n",
    "    x0 = np.zeros(n)\n",
    "\n",
    "    X1 = gradient_descent(A, b, x0, n)\n",
    "    X2, D, R, alphas, betas, n_residuals = conjugate_gradient(A, b, x0, True)\n",
    "    r_sol = np.linalg.solve(A, b)\n",
    "\n",
    "    E1 = relative_error(X1, r_sol)\n",
    "    E2 = relative_error(X2, r_sol)\n",
    "    iterations1 = np.linspace(1, n, n)\n",
    "    iterations2 = np.linspace(1, X2.shape[0], X2.shape[0])\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Relative Error')\n",
    "    plt.title('Evolution of the Relative Forward Error for each method')\n",
    "    plt.semilogy(iterations1, E1, 'rd', markersize=8, label='GD') # Red diamonds are for Gradient Descent\n",
    "    plt.semilogy(iterations2, E2, 'b.', markersize=8, label='CG') # Blue dots are for Conjugate Gradient\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "interact(show_output_for_non_symmetric_and_npd,np_seed=(0,100,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, if the matrix doesn't meet the requirements for these methods, the results can be quite terrible.\n",
    "\n",
    "Let's try again, this time using an appropriate matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56852485fff4414bbbdefdea8807b8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='np_seed'), IntSlider(value=100, description='n', max=100…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_output_for_symmetric_and_pd(np_seed=0, n=100)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_output_for_symmetric_and_pd(np_seed=0,n=100):\n",
    "    np.random.seed(np_seed)\n",
    "    A = generate_spd_matrix(n)\n",
    "    b = np.random.random(n)\n",
    "    x0 = np.zeros(n)\n",
    "\n",
    "    X1 = gradient_descent(A, b, x0, n)\n",
    "    X2, D, R, alphas, betas, n_residuals = conjugate_gradient(A, b, x0, True)\n",
    "    r_sol = np.linalg.solve(A, b)\n",
    "\n",
    "    E1 = relative_error(X1, r_sol)\n",
    "    E2 = relative_error(X2, r_sol)\n",
    "    iterations1 = np.linspace(1, n, n)\n",
    "    iterations2 = np.linspace(1, X2.shape[0], X2.shape[0])\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Relative Error')\n",
    "    plt.title('Evolution of the Relative Forward Error for each method')\n",
    "    plt.semilogy(iterations1, E1, 'rd', markersize=8, label='GD') # Red diamonds are for Gradient Descent\n",
    "    plt.semilogy(iterations2, E2, 'b.', markersize=8, label='CG') # Blue dots are for Conjugate Gradient\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlim([0,40])\n",
    "    plt.show()\n",
    "interact(show_output_for_symmetric_and_pd,np_seed=(0,100,1),n=(10,1000,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! We started with a huge relative error and reduced it to practically zero in just under 10 iterations (the algorithms all have 100 iterations but we're showing you the first 40). \n",
    "We can clearly see that the Conjugate Gradient Method converges faster than the Gradient Descent method, even for larger matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, reached a certain size for the matrix, the amount of iterations needed to reach a small error remains more or less the same. We encourage you to try other kinds of matrices to see how the algorithms behave, and experiment with the code. Now let's move on to profiling.\n",
    "\n",
    "Of course, you win some, you lose some. Accelerating the convergence of the algorithm means you have to spend more of other resources. We'll use the functions `%timeit` and `%memit` to see how the algorithms behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = generate_spd_matrix(100)\n",
    "b = np.ones(100)\n",
    "x0 = np.random.random(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.9 ms ± 6.79 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "The slowest run took 14.05 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "58 ms ± 52.3 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit gradient_descent(A, b, x0, n_iter=100, tol=1e-5)\n",
    "%timeit conjugate_gradient(A, b, x0, tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented because it is taking too long, we need to review this!\n",
    "# %memit gradient_descent(A, b, x0, n_iter=100, tol=1e-5)\n",
    "# %memit conjugate_gradient(A, b, x0, tol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see something interesting here: all algorithms need about the same amount of memory.\n",
    "\n",
    "What happened with the measure of time? Why is it so big for the algorithm that has the best convergence rate? Besides the end of the loop, we have one other criteria for stopping the algorithm: When the residue r reaches the _exact_ value of zero, we say that the algorithm converged, and stop. However it's very hard to get an error of zero for randomized initial guesses, so this almost never happens, and we can't take advantage of the convergence rate of the algorithms. \n",
    "\n",
    "There's a way we can fix this: instead of using this criteria, make the algorithm stop when a certain _tolerance_ or _threshold_ is reached. That way, when the error gets small enough, we can stop and say that we got a good enough solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try with different matrices, different initial conditions, different sizes, etc. Try some more plotting, profiling, and experimenting. Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='acknowledgements' />\n",
    "\n",
    "# Acknowledgements\n",
    "\n",
    "* _Material created by professor Claudio Torres_ (`ctorres@inf.utfsm.cl`) _and assistants: Laura Bermeo, Alvaro Salinas, Axel Simonsen and Martín Villanueva. DI UTFSM. April 2016._\n",
    "* _Modified by professor Claudio Torres_ (`ctorres@inf.utfsm.cl`). _DI UTFSM. April 2019._\n",
    "* _Update May 2020 - v1.15 - C.Torres_ : Fixing formatting issues.\n",
    "* _Update June 2020 - v1.16 - C.Torres_ : Adding 'compute_A_orthogonality' and extending 'show_small_example_CG'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
